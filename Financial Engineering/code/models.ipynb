{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from itertools import product\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先從總資料抽20%(每年抽等比例的等分)並把這20%隨機分成10%的train跟10%的test(同時上升跟下降的樣本數要一樣)，用這20%的資料去train出最佳hyper parameters，最後在比較用各模型的最佳hyper parameters來跑全資料，並比較各模型的表現(f1, accuracy)。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_df_for_model(df_price, df_tech):\n",
    "    df_price['diff'] = df_price['close'].diff().dropna()\n",
    "    df_price = df_price[df_price['diff'] != 0]\n",
    "    df_price['diff'] = df_price['diff'].apply(lambda x : -1 if x<0 else 1)\n",
    "    df_price_diff = df_price['diff']\n",
    "    df_tech.Date = pd.to_datetime(df_tech.Date, format = '%Y/%m/%d')\n",
    "    df_tech = df_tech.set_index('Date')\n",
    "    df_merge = pd.merge(df_price_diff, df_tech, left_index=True, right_index=True, how='inner')\n",
    "    return df_merge\n",
    "\n",
    "def spilit_for_par(df):\n",
    "\n",
    "    df['Year'] = df.index.year\n",
    "    samples_per_year = df.groupby('Year').size()\n",
    "\n",
    "    training = []\n",
    "    testing = []\n",
    "\n",
    "    for year, days in zip(df['Year'].unique(), samples_per_year):\n",
    "        d = int(days*0.1)\n",
    "        year_data = df[df['Year'] == year]\n",
    "        \n",
    "        up_samples = year_data[year_data['diff'] == 1].sample(d)\n",
    "        up_training = up_samples.sample(n = int(len(up_samples)/2))\n",
    "        up_testing = up_samples.copy().drop(up_training.index)\n",
    "        \n",
    "        down_samples = year_data[year_data['diff'] == -1].sample(d)\n",
    "        down_training = down_samples.sample(n = int(len(down_samples)/2))\n",
    "        down_testing = down_samples.copy().drop(down_training.index)\n",
    "        \n",
    "        training.append(pd.concat([up_training, down_training]))\n",
    "        testing.append(pd.concat([up_testing, down_testing]))\n",
    "\n",
    "    final_training = pd.concat(training)\n",
    "    final_testing = pd.concat(testing)\n",
    "\n",
    "    final_training = final_training.drop('Year', axis=1)\n",
    "    final_testing = final_testing.drop('Year', axis=1)\n",
    "    return final_training, final_testing\n",
    "\n",
    "def spilit_for_off(df):\n",
    "\n",
    "    df['Year'] = df.index.year\n",
    "    training = []\n",
    "    testing = []\n",
    "\n",
    "    for year in df['Year'].unique():\n",
    "        year_data = df[df['Year'] == year]\n",
    "        \n",
    "        up_samples = year_data[year_data['diff'] == 1]\n",
    "        up_training = up_samples.sample(frac = 0.5)\n",
    "        up_testing = up_samples.copy().drop(up_training.index)\n",
    "        \n",
    "        down_samples = year_data[year_data['diff'] == -1]\n",
    "        down_training = down_samples.sample(frac = 0.5)\n",
    "        down_testing = down_samples.copy().drop(down_training.index)\n",
    "        \n",
    "        training.append(pd.concat([up_training, down_training]))\n",
    "        testing.append(pd.concat([up_testing, down_testing]))\n",
    "\n",
    "    final_training = pd.concat(training)\n",
    "    final_testing = pd.concat(testing)\n",
    "\n",
    "    final_training = final_training.drop('Year', axis=1)\n",
    "    final_testing = final_testing.drop('Year', axis=1)\n",
    "    return final_training, final_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN\n",
    "# def ann(X_train, y_train, X_test, y_test, par = []):\n",
    "\n",
    "#     # parameters \n",
    "#     if len(par) == 0:\n",
    "#         n = list(np.arange(10, 101, 10))\n",
    "#         mc = list(np.arange(0.1, 1, 0.1))\n",
    "#         ep = list(np.arange(1000, 10001, 1000))\n",
    "#     else : \n",
    "#         n = [par[0]]\n",
    "#         mc = [par[1]]\n",
    "#         ep = [par[2]]\n",
    "\n",
    "#     lr = 0.1\n",
    "\n",
    "#     score_ann = pd.DataFrame(columns = ['hidden layer neurons', 'momentum constant', 'epochs', 'accuracy', 'f1 score'])\n",
    "#     for combine in product(n, mc, ep):\n",
    "#         n = combine[0]\n",
    "#         mc = combine[1]\n",
    "#         ep = combine[2]\n",
    "\n",
    "#         input_size = 10  \n",
    "#         hidden_size = n  \n",
    "#         output_size = 1  \n",
    "\n",
    "#         model = models.Sequential([\n",
    "#             layers.Dense(hidden_size, activation = 'tanh', input_shape = (input_size,)),\n",
    "#             layers.Dense(output_size, activation = 'sigmoid')\n",
    "#         ])\n",
    "\n",
    "#         model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum = mc),\n",
    "#                     loss='binary_crossentropy',\n",
    "#                     metrics=['accuracy'])\n",
    "\n",
    "#         model.fit(X_train, y_train, epochs=int(ep), batch_size = 32, validation_split = 0) # 不確定 batch_size 跟 vs 要設多少，就先用default\n",
    "\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         y_pred_binary = pd.Series((y_pred >= 0.5).astype(int).ravel()).apply(lambda x : 1 if x>= 0.5 else -1)\n",
    "\n",
    "#         accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "#         f1 = f1_score(y_test, y_pred_binary)\n",
    "#         score_ann.loc[len(score_ann)] = [n, mc, ep, accuracy, f1]\n",
    "#         #print(f\"Accuracy : {accuracy} /// F1 score : {f1}\")\n",
    "#     return score_ann\n",
    "\n",
    "def ann(X_train, y_train, X_test, y_test, par=[]):\n",
    "    if len(par) == 0:\n",
    "        n = list(np.arange(10, 101, 10))\n",
    "        mc = list(np.arange(0.1, 1, 0.1))\n",
    "        ep = list(np.arange(1000, 10001, 1000))\n",
    "    else:\n",
    "        n = [par[0]]\n",
    "        mc = [par[1]]\n",
    "        ep = [par[2]]\n",
    "\n",
    "    lr = 0.1\n",
    "\n",
    "    score_ann = pd.DataFrame(columns=['hidden layer neurons', 'momentum constant', 'epochs', 'accuracy', 'f1 score'])\n",
    "    for combine in product(n, mc, ep):\n",
    "        n_val = combine[0]\n",
    "        mc_val = combine[1]\n",
    "        ep_val = combine[2]\n",
    "        \n",
    "        model = MLPClassifier(\n",
    "            hidden_layer_sizes=(int(n_val),),  # 只有一層 hidden layer\n",
    "            activation='tanh',  # tanh 作為激活函數\n",
    "            solver='sgd',  # 隨機梯度下降優化器\n",
    "            learning_rate_init=lr,\n",
    "            momentum=mc_val,\n",
    "            max_iter=int(ep_val),\n",
    "            random_state=42  # 設定隨機種子以保證結果可重現\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        score_ann.loc[len(score_ann)] = [n_val, mc_val, ep_val, accuracy, f1]\n",
    "    return score_ann\n",
    "    \n",
    "# SVM (polynomial)\n",
    "def svm_p(X_train, y_train, X_test, y_test, par = []):\n",
    "    # parameters \n",
    "    if len(par) == 0:\n",
    "        degree = [1, 2, 3, 4]\n",
    "        reg_para = [0.5, 1, 5, 10, 100] \n",
    "    else:\n",
    "        degree = [par[0]]\n",
    "        reg_para = [par[1]] \n",
    "\n",
    "    score_svm_p = pd.DataFrame(columns = ['Degree of kernel function', 'Regularization parameter', 'accuracy', 'f1 score'])\n",
    "\n",
    "    for combine in product(degree ,reg_para):\n",
    "        d = combine[0]\n",
    "        c = combine[1]\n",
    "        poly_svm = svm.SVC(kernel='poly', degree=int(d), C=c)\n",
    "        poly_svm.fit(X_train, y_train)\n",
    "        y_pred_poly = poly_svm.predict(X_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred_poly)\n",
    "        f1 = f1_score(y_test, y_pred_poly)\n",
    "\n",
    "        score_svm_p.loc[len(score_svm_p)] = [d, c, accuracy, f1]\n",
    "    return score_svm_p\n",
    "\n",
    "# SVM (radial basis)\n",
    "def svm_r(X_train, y_train, X_test, y_test, par = []):\n",
    "    # parameters \n",
    "    if len(par) == 0:\n",
    "        gamma = list(np.arange(0.5, 5.1, 0.5)) + [10] # 這邊paper超怪，他寫這樣總共有10個，幹但明明就11個，大概是typo\n",
    "        reg_para = [0.5, 1, 5, 10] \n",
    "    else:\n",
    "        gamma = [par[0]]\n",
    "        reg_para = [par[1]]\n",
    "    \n",
    "    score_svm_r = pd.DataFrame(columns = ['Gamma in kernel function', 'Regularization parameter', 'accuracy', 'f1 score'])\n",
    "    for combine in product(gamma ,reg_para):\n",
    "        g = combine[0]\n",
    "        c = combine[1]\n",
    "        rbf_svm = svm.SVC(kernel='rbf', C = c, gamma = float(g))\n",
    "        rbf_svm.fit(X_train, y_train)\n",
    "        y_pred_rbf = rbf_svm.predict(X_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred_rbf)\n",
    "        f1 = f1_score(y_test, y_pred_rbf)\n",
    "\n",
    "        score_svm_r.loc[len(score_svm_r)] = [g, c, accuracy, f1]\n",
    "    return score_svm_r\n",
    "\n",
    "# RF\n",
    "def rf(X_train, y_train, X_test, y_test, par = []):\n",
    "    # parameters\n",
    "    if len(par)==0:\n",
    "        ntree = list(np.arange(10, 201, 10))\n",
    "    else:\n",
    "        ntree = [par[0]]\n",
    "    score_rf = pd.DataFrame(columns = ['n', 'accuracy', 'f1 score'])\n",
    "    for n in ntree:\n",
    "        \n",
    "        random_forest = RandomForestClassifier(n_estimators = int(n), random_state = 5278)\n",
    "\n",
    "        random_forest.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_rf = random_forest.predict(X_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "        f1 = f1_score(y_test, y_pred_rf)\n",
    "        score_rf.loc[len(score_rf)] = [n, accuracy, f1]\n",
    "    return score_rf\n",
    "\n",
    "# NB\n",
    "def nb(X_train, y_train, X_test, y_test, flag):\n",
    "    if flag == 1:\n",
    "        naive_bayes = GaussianNB() # for 連續特徵\n",
    "    elif flag == 2:\n",
    "        naive_bayes = BernoulliNB() # for 離散特徵\n",
    "    \n",
    "    naive_bayes.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_nb = naive_bayes.predict(X_test)\n",
    "\n",
    "    accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "    f1_nb = f1_score(y_test, y_pred_nb)\n",
    "    return [accuracy_nb, f1_nb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ann = {\n",
    "    'hidden_size': list(np.arange(10, 101, 10)),\n",
    "    'momentum': list(np.arange(0.1, 1, 0.1)),\n",
    "    'epochs': list(np.arange(1000, 10001, 1000))\n",
    "}\n",
    "\n",
    "\n",
    "param_grid_svm_p = {\n",
    "    'degree': [1, 2, 3, 4],\n",
    "    'C': [0.5, 1, 5, 10, 100]\n",
    "}\n",
    "\n",
    "param_grid_svm_r = {\n",
    "    'gamma': list(np.arange(0.5, 5.1, 0.5)) + [10],\n",
    "    'C': [0.5, 1, 5, 10]\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': list(np.arange(10, 201, 10))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method_1_0050.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_20460\\2445863983.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_price['diff'] = df_price['diff'].apply(lambda x : -1 if x<0 else 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method_1_2330.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_20460\\2445863983.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_price['diff'] = df_price['diff'].apply(lambda x : -1 if x<0 else 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method_1_2881.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_20460\\2445863983.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_price['diff'] = df_price['diff'].apply(lambda x : -1 if x<0 else 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method_1_TX.csv\n",
      "method_2_0050.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_20460\\2445863983.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_price['diff'] = df_price['diff'].apply(lambda x : -1 if x<0 else 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method_2_2330.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_20460\\2445863983.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_price['diff'] = df_price['diff'].apply(lambda x : -1 if x<0 else 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method_2_2881.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_20460\\2445863983.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_price['diff'] = df_price['diff'].apply(lambda x : -1 if x<0 else 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method_2_TX.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from itertools import product\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_raw = pd.read_excel('../data/data.xlsx', index_col = 0, header=1)\n",
    "df_raw.dropna(inplace=True)\n",
    "df_raw.index = pd.to_datetime(df_raw.index)\n",
    "col_name = ['open', 'high', 'low', 'close', 'vol']\n",
    "df_TX = df_raw.iloc[:,:5].set_axis(col_name, axis = 1)\n",
    "df_0050 = df_raw.iloc[:,5:10].set_axis(col_name, axis = 1)\n",
    "df_2330 = df_raw.iloc[:,10:15].set_axis(col_name, axis = 1)\n",
    "df_2881 = df_raw.iloc[:,15:20].set_axis(col_name, axis = 1)\n",
    "\n",
    "data_input = '../data/tidy'\n",
    "data_output_par = '../data/output/parameters'\n",
    "data_output_off = '../data/output/official'\n",
    "data_output_off_ra = '../data/output/official_random_split'\n",
    "\n",
    "for file in os.listdir(data_input):\n",
    "    print(file)\n",
    "    df_tech = pd.read_csv(os.path.join(data_input, file))\n",
    "\n",
    "    # com\n",
    "    if '0050' in file:\n",
    "        df_price = df_0050.copy()\n",
    "    elif '2330' in file:\n",
    "        df_price = df_2330.copy()\n",
    "    elif '2881' in file:\n",
    "        df_price = df_2881.copy()\n",
    "    elif 'TX' in file:\n",
    "        df_price = df_TX.copy()\n",
    "    \n",
    "    # flag for nb\n",
    "    if file.split('_')[1] == '1':\n",
    "        flag = 1\n",
    "    elif file.split('_')[1] == '2':\n",
    "        flag = 2\n",
    "\n",
    "    df_all = combined_df_for_model(df_price, df_tech)\n",
    "    # for training parameters\n",
    "    training_par, testing_par = spilit_for_par(df_all)\n",
    "    X_train_par = training_par.iloc[:, 1:]\n",
    "    y_train_par = training_par.iloc[:, 0]\n",
    "    X_test_par = testing_par.iloc[:, 1:]\n",
    "    y_test_par = testing_par.iloc[:, 0]\n",
    "\n",
    "    # for official parameters\n",
    "    training, testing = spilit_for_off(df_all)\n",
    "    X_train = training.iloc[:, 1:]\n",
    "    y_train = training.iloc[:, 0]\n",
    "    X_test = testing.iloc[:, 1:]\n",
    "    y_test = testing.iloc[:, 0]\n",
    "\n",
    "    # for randomly split\n",
    "    X_train_ra, X_test_ra, y_train_ra, y_test_ra = train_test_split(df_all.iloc[:, 1:], df_all.iloc[:, 0], test_size=0.5, random_state=5278)\n",
    "\n",
    "    df_ann_par = ann(X_train_par, y_train_par, X_test_par, y_test_par)\n",
    "    df_ann_par.to_csv(os.path.join(data_output_par, file.replace('.csv', '') + '_ann.csv'), index = False)\n",
    "    ann_temp = df_ann_par[df_ann_par['accuracy'] == max(df_ann_par['accuracy'])]\n",
    "    ann_par = [ann_temp['hidden layer neurons'].values[0], ann_temp['momentum constant'].values[0], ann_temp['epochs'].values[0]]\n",
    "    df_ann = ann(X_train, y_train, X_test, y_test, ann_par)\n",
    "    df_ann.to_csv(os.path.join(data_output_off, file.replace('.csv', '') + '_ann.csv'), index = False)\n",
    "    df_ann_ra = ann(X_train_ra, y_train_ra, X_test_ra, y_test_ra, ann_par)\n",
    "    df_ann_ra.to_csv(os.path.join(data_output_off_ra, file.replace('.csv', '') + '_ann.csv'), index = False)\n",
    "\n",
    "    df_svm_p_par = svm_p(X_train_par, y_train_par, X_test_par, y_test_par)\n",
    "    df_svm_p_par.to_csv(os.path.join(data_output_par, file.replace('.csv', '') + '_svm_p.csv'), index = False)\n",
    "    svm_p_temp = df_svm_p_par[df_svm_p_par['accuracy'] == max(df_svm_p_par['accuracy'])]\n",
    "    svm_p_par = [svm_p_temp['Degree of kernel function'].values[0], svm_p_temp['Regularization parameter'].values[0]]\n",
    "    df_svm_p = svm_p(X_train, y_train, X_test, y_test, svm_p_par)\n",
    "    df_svm_p.to_csv(os.path.join(data_output_off, file.replace('.csv', '') + '_svm_p.csv'), index = False)\n",
    "    df_svm_p_ra = svm_p(X_train_ra, y_train_ra, X_test_ra, y_test_ra, svm_p_par)\n",
    "    df_svm_p_ra.to_csv(os.path.join(data_output_off_ra, file.replace('.csv', '') + '_svm_p.csv'), index = False)\n",
    "\n",
    "    df_svm_r_par = svm_r(X_train_par, y_train_par, X_test_par, y_test_par)\n",
    "    df_svm_r_par.to_csv(os.path.join(data_output_par, file.replace('.csv', '') + '_svm_r.csv'), index = False)\n",
    "    svm_r_temp = df_svm_r_par[df_svm_r_par['accuracy'] == max(df_svm_r_par['accuracy'])]\n",
    "    svm_r_par = [svm_r_temp['Gamma in kernel function'].values[0], svm_r_temp['Regularization parameter'].values[0]]\n",
    "    df_svm_r = svm_r(X_train, y_train, X_test, y_test, svm_r_par)\n",
    "    df_svm_r.to_csv(os.path.join(data_output_off, file.replace('.csv', '') + '_svm_r.csv'), index = False)\n",
    "    df_svm_r_ra = svm_r(X_train_ra, y_train_ra, X_test_ra, y_test_ra, svm_r_par)\n",
    "    df_svm_r_ra.to_csv(os.path.join(data_output_off_ra, file.replace('.csv', '') + '_svm_r.csv'), index = False)\n",
    "\n",
    "    df_rf_par = rf(X_train_par, y_train_par, X_test_par, y_test_par)\n",
    "    df_rf_par.to_csv(os.path.join(data_output_par, file.replace('.csv', '') + '_rf.csv'), index = False)\n",
    "    rf_temp = df_rf_par[df_rf_par['accuracy'] == max(df_rf_par['accuracy'])]\n",
    "    rf_par = [rf_temp['n'].values[0]]\n",
    "    df_rf = rf(X_train, y_train, X_test, y_test, rf_par)\n",
    "    df_rf.to_csv(os.path.join(data_output_off, file.replace('.csv', '') + '_rf.csv'), index = False)\n",
    "    df_rf_ra = rf(X_train_ra, y_train_ra, X_test_ra, y_test_ra, rf_par)\n",
    "    df_rf_ra.to_csv(os.path.join(data_output_off_ra, file.replace('.csv', '') + '_rf.csv'), index = False)\n",
    "\n",
    "    df_nb = pd.DataFrame(nb(X_train, y_train, X_test, y_test, flag)) # nb no parameters\n",
    "    df_nb.to_csv(os.path.join(data_output_off, file.replace('.csv', '') + '_nb.csv'), index = False)\n",
    "    df_nb_ra = pd.DataFrame(nb(X_train_ra, y_train_ra, X_test_ra, y_test_ra, flag)) # nb no parameters\n",
    "    df_nb_ra.to_csv(os.path.join(data_output_off_ra, file.replace('.csv', '') + '_nb.csv'), index = False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
